{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from requests) (2025.6.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from bs4) (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from beautifulsoup4->bs4) (4.14.0)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: chromadb in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (1.0.13)\n",
      "Requirement already satisfied: build>=1.0.3 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (2.11.7)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (1.4.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (2.3.1)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (4.14.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (0.21.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (1.73.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from chromadb) (4.24.0)\n",
      "Requirement already satisfied: packaging>=19.1 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (25.0)\n",
      "Requirement already satisfied: pyproject_hooks in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: anyio in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
      "Requirement already satisfied: requests-oauthlib in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: coloredlogs in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.34.1 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.55b1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb) (0.33.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.5)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /home/rcsen/Documents/python/ibm_hackathon/venv/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests\n",
    "%pip install bs4\n",
    "%pip install tqdm\n",
    "%pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from tqdm import tqdm\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import sys\n",
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('scraper.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "visited_urls = set()\n",
    "\n",
    "class WebScraperWithVector:\n",
    "    def __init__(self, collection_name=\"web_content\", persist_directory=\"./chroma_db\"):\n",
    "        \"\"\"Initialize the scraper with ChromaDB integration\"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self.setup_chromadb()\n",
    "        \n",
    "    def setup_chromadb(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            logger.info(\"üîß Initializing ChromaDB...\")\n",
    "            \n",
    "            # Create persistent directory\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            \n",
    "            # Initialize ChromaDB client with persistence\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            try:\n",
    "                self.collection = self.client.get_collection(name=self.collection_name)\n",
    "                logger.info(f\"üìö Found existing collection: {self.collection_name}\")\n",
    "                logger.info(f\"üìä Collection contains {self.collection.count()} documents\")\n",
    "            except:\n",
    "                self.collection = self.client.create_collection(\n",
    "                    name=self.collection_name,\n",
    "                    metadata={\"description\": \"Web scraped content with semantic search\"}\n",
    "                )\n",
    "                logger.info(f\"üÜï Created new collection: {self.collection_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to initialize ChromaDB: {e}\")\n",
    "            raise\n",
    "\n",
    "    def fetch_html(self, url):\n",
    "        \"\"\"Fetch HTML content from URL\"\"\"\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "            }\n",
    "            response = requests.get(url, timeout=10, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to fetch {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def is_valid_url(self, url):\n",
    "        \"\"\"Check if URL is valid and not blacklisted\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        blacklisted_domains = [\"linkedin.com\", \"facebook.com\", \"twitter.com\", \"instagram.com\"]\n",
    "        return (parsed.scheme in [\"http\", \"https\"] and \n",
    "                not any(domain in parsed.netloc for domain in blacklisted_domains))\n",
    "\n",
    "    def extract_links(self, soup, base_url):\n",
    "        \"\"\"Extract all valid links from the page\"\"\"\n",
    "        links = set()\n",
    "        for tag in soup.find_all(\"a\", href=True):\n",
    "            href = tag.get(\"href\")\n",
    "            full_url = urljoin(base_url, href)\n",
    "            if self.is_valid_url(full_url):\n",
    "                links.add(full_url)\n",
    "        return links\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text content\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        # Remove extra whitespace and normalize\n",
    "        return ' '.join(text.split()).strip()\n",
    "\n",
    "    def parse_content(self, url):\n",
    "        \"\"\"Parse content from a single URL\"\"\"\n",
    "        html = self.fetch_html(url)\n",
    "        if html is None:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "\n",
    "        # Extract structured data\n",
    "        data = {\n",
    "            \"url\": url,\n",
    "            \"title\": self.clean_text(soup.title.string) if soup.title and soup.title.string else \"\",\n",
    "            \"headings\": [self.clean_text(h.get_text()) for h in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']) if h.get_text().strip()],\n",
    "            \"paragraphs\": [self.clean_text(p.get_text()) for p in soup.find_all(\"p\") if p.get_text().strip()],\n",
    "            \"body_text\": self.clean_text(soup.get_text()),\n",
    "            \"images\": [{\"src\": urljoin(url, img.get(\"src\")), \"alt\": img.get(\"alt\", \"\")} \n",
    "                      for img in soup.find_all(\"img\") if img.get(\"src\")],\n",
    "            \"links\": list(self.extract_links(soup, url)),\n",
    "            \"scraped_at\": datetime.now().isoformat(),\n",
    "            \"content_hash\": None\n",
    "        }\n",
    "        \n",
    "        # Generate content hash for deduplication\n",
    "        content_for_hash = f\"{data['title']}{' '.join(data['headings'])}{' '.join(data['paragraphs'])}\"\n",
    "        data[\"content_hash\"] = hashlib.md5(content_for_hash.encode()).hexdigest()\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def prepare_document_for_vector_db(self, page_data):\n",
    "        \"\"\"Prepare document content and metadata for ChromaDB\"\"\"\n",
    "        # Combine all text content for embedding\n",
    "        text_content = []\n",
    "        \n",
    "        if page_data.get(\"title\"):\n",
    "            text_content.append(f\"Title: {page_data['title']}\")\n",
    "            \n",
    "        if page_data.get(\"headings\"):\n",
    "            text_content.append(f\"Headings: {' | '.join(page_data['headings'])}\")\n",
    "            \n",
    "        if page_data.get(\"paragraphs\"):\n",
    "            # Limit paragraph content to avoid token limits\n",
    "            paragraphs = page_data['paragraphs'][:10]  # First 10 paragraphs\n",
    "            text_content.append(f\"Content: {' '.join(paragraphs)}\")\n",
    "        \n",
    "        document_text = ' '.join(text_content)\n",
    "        \n",
    "        # Prepare metadata\n",
    "        metadata = {\n",
    "            \"url\": page_data[\"url\"],\n",
    "            \"title\": page_data.get(\"title\", \"\")[:100],  # Truncate for metadata limits\n",
    "            \"scraped_at\": page_data[\"scraped_at\"],\n",
    "            \"content_hash\": page_data[\"content_hash\"],\n",
    "            \"num_headings\": len(page_data.get(\"headings\", [])),\n",
    "            \"num_paragraphs\": len(page_data.get(\"paragraphs\", [])),\n",
    "            \"num_images\": len(page_data.get(\"images\", [])),\n",
    "            \"num_links\": len(page_data.get(\"links\", []))\n",
    "        }\n",
    "        \n",
    "        return document_text, metadata\n",
    "\n",
    "    def insert_to_vector_db(self, structured_data):\n",
    "        \"\"\"Insert scraped data into ChromaDB\"\"\"\n",
    "        logger.info(\"üíæ Inserting data into vector database...\")\n",
    "        \n",
    "        documents = []\n",
    "        metadatas = []\n",
    "        ids = []\n",
    "        \n",
    "        for i, page_data in enumerate(tqdm(structured_data, desc=\"Preparing documents\")):\n",
    "            try:\n",
    "                document_text, metadata = self.prepare_document_for_vector_db(page_data)\n",
    "                \n",
    "                # Skip empty documents\n",
    "                if not document_text.strip():\n",
    "                    logger.warning(f\"‚ö†Ô∏è Skipping empty document: {page_data.get('url', 'Unknown')}\")\n",
    "                    continue\n",
    "                \n",
    "                # Generate unique ID\n",
    "                doc_id = f\"doc_{page_data['content_hash']}_{i}\"\n",
    "                \n",
    "                documents.append(document_text)\n",
    "                metadatas.append(metadata)\n",
    "                ids.append(doc_id)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Error preparing document {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not documents:\n",
    "            logger.warning(\"‚ö†Ô∏è No valid documents to insert\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Insert in batches to avoid memory issues\n",
    "            batch_size = 50\n",
    "            total_inserted = 0\n",
    "            \n",
    "            for i in range(0, len(documents), batch_size):\n",
    "                batch_docs = documents[i:i+batch_size]\n",
    "                batch_metas = metadatas[i:i+batch_size]\n",
    "                batch_ids = ids[i:i+batch_size]\n",
    "                \n",
    "                self.collection.add(\n",
    "                    documents=batch_docs,\n",
    "                    metadatas=batch_metas,\n",
    "                    ids=batch_ids\n",
    "                )\n",
    "                \n",
    "                total_inserted += len(batch_docs)\n",
    "                logger.info(f\"üìù Inserted batch: {len(batch_docs)} documents (Total: {total_inserted})\")\n",
    "            \n",
    "            logger.info(f\"‚úÖ Successfully inserted {total_inserted} documents into ChromaDB\")\n",
    "            logger.info(f\"üìä Total collection size: {self.collection.count()} documents\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to insert documents: {e}\")\n",
    "            raise\n",
    "\n",
    "    def search_similar_content(self, query, n_results=5):\n",
    "        \"\"\"Search for similar content in the vector database\"\"\"\n",
    "        try:\n",
    "            results = self.collection.query(\n",
    "                query_texts=[query],\n",
    "                n_results=n_results\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"üîç Found {len(results['documents'][0])} results for query: '{query}'\")\n",
    "            \n",
    "            for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
    "                print(f\"\\n--- Result {i+1} ---\")\n",
    "                print(f\"URL: {metadata['url']}\")\n",
    "                print(f\"Title: {metadata['title']}\")\n",
    "                print(f\"Content Preview: {doc[:200]}...\")\n",
    "                print(f\"Scraped: {metadata['scraped_at']}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Search failed: {e}\")\n",
    "\n",
    "    def crawl_website(self, start_url, max_depth=2, max_pages=None):\n",
    "        \"\"\"Crawl website and collect structured data\"\"\"\n",
    "        structured_data = []\n",
    "        to_visit = [(start_url, 0)]\n",
    "        visited_urls.add(start_url)\n",
    "        pages_processed = 0\n",
    "\n",
    "        logger.info(f\"üöÄ Starting crawl from: {start_url}\")\n",
    "        logger.info(f\"üìê Max depth: {max_depth}, Max pages: {max_pages or 'unlimited'}\")\n",
    "\n",
    "        while to_visit and (max_pages is None or pages_processed < max_pages):\n",
    "            current_url, depth = to_visit.pop(0)\n",
    "            if depth > max_depth:\n",
    "                continue\n",
    "\n",
    "            logger.info(f\"üîç Parsing [depth {depth}]: {current_url}\")\n",
    "            page_data = self.parse_content(current_url)\n",
    "            \n",
    "            if page_data:\n",
    "                structured_data.append(page_data)\n",
    "                pages_processed += 1\n",
    "                \n",
    "                # Add new links to visit queue\n",
    "                new_links = 0\n",
    "                for link in page_data[\"links\"]:\n",
    "                    if link not in visited_urls and urlparse(link).netloc == urlparse(start_url).netloc:\n",
    "                        visited_urls.add(link)\n",
    "                        to_visit.append((link, depth + 1))\n",
    "                        new_links += 1\n",
    "                \n",
    "                logger.info(f\"‚úÖ Processed page (found {new_links} new links)\")\n",
    "            else:\n",
    "                logger.warning(f\"‚ö†Ô∏è Failed to process: {current_url}\")\n",
    "\n",
    "        logger.info(f\"üéØ Crawl completed: {len(structured_data)} pages processed\")\n",
    "        return structured_data\n",
    "\n",
    "    def save_structured_data(self, data, output_dir=\"scraped_data\"):\n",
    "        \"\"\"Save structured data to JSON files\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Save individual page files\n",
    "        for i, page in enumerate(data):\n",
    "            filename = os.path.join(output_dir, f\"page_{i+1}.json\")\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(page, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        # Save combined data\n",
    "        combined_file = os.path.join(output_dir, \"all_pages.json\")\n",
    "        with open(combined_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        logger.info(f\"üíæ Saved {len(data)} pages to '{output_dir}'\")\n",
    "\n",
    "    def get_collection_stats(self):\n",
    "        \"\"\"Display collection statistics\"\"\"\n",
    "        try:\n",
    "            count = self.collection.count()\n",
    "            logger.info(f\"üìä Collection Statistics:\")\n",
    "            logger.info(f\"   Collection Name: {self.collection_name}\")\n",
    "            logger.info(f\"   Total Documents: {count}\")\n",
    "            logger.info(f\"   Storage Path: {self.persist_directory}\")\n",
    "            \n",
    "            if count > 0:\n",
    "                # Get a sample document to show structure\n",
    "                sample = self.collection.peek(limit=1)\n",
    "                if sample['metadatas']:\n",
    "                    logger.info(f\"   Sample Metadata Keys: {list(sample['metadatas'][0].keys())}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to get collection stats: {e}\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Web Scraper with ChromaDB Vector Storage\")\n",
    "    parser.add_argument(\"--url\", required=True, help=\"Starting URL to scrape\")\n",
    "    parser.add_argument(\"--depth\", type=int, default=2, help=\"Maximum crawl depth\")\n",
    "    parser.add_argument(\"--max-pages\", type=int, help=\"Maximum pages to scrape\")\n",
    "    parser.add_argument(\"--collection\", default=\"web_content\", help=\"ChromaDB collection name\")\n",
    "    parser.add_argument(\"--db-path\", default=\"./chroma_db\", help=\"ChromaDB storage path\")\n",
    "    parser.add_argument(\"--output-dir\", default=\"scraped_data\", help=\"Output directory for JSON files\")\n",
    "    parser.add_argument(\"--search\", help=\"Search query to test vector database\")\n",
    "    parser.add_argument(\"--stats\", action=\"store_true\", help=\"Show collection statistics\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    try:\n",
    "        # Initialize scraper\n",
    "        scraper = WebScraperWithVector(\n",
    "            collection_name=args.collection,\n",
    "            persist_directory=args.db_path\n",
    "        )\n",
    "        \n",
    "        # Show stats if requested\n",
    "        if args.stats:\n",
    "            scraper.get_collection_stats()\n",
    "            return\n",
    "        \n",
    "        # Perform search if query provided\n",
    "        if args.search:\n",
    "            scraper.search_similar_content(args.search)\n",
    "            return\n",
    "        \n",
    "        # Crawl website\n",
    "        logger.info(\"üåê Starting web scraping process...\")\n",
    "        structured_data = scraper.crawl_website(\n",
    "            start_url=args.url,\n",
    "            max_depth=args.depth,\n",
    "            max_pages=args.max_pages\n",
    "        )\n",
    "        \n",
    "        if not structured_data:\n",
    "            logger.warning(\"‚ö†Ô∏è No data was scraped\")\n",
    "            return\n",
    "        \n",
    "        # Save to files\n",
    "        scraper.save_structured_data(structured_data, args.output_dir)\n",
    "        \n",
    "        # Insert into vector database\n",
    "        scraper.insert_to_vector_db(structured_data)\n",
    "        \n",
    "        # Show final statistics\n",
    "        scraper.get_collection_stats()\n",
    "        \n",
    "        logger.info(\"üéâ Scraping and vector storage completed successfully!\")\n",
    "        logger.info(f\"üí° Try searching with: python script.py --search 'your query here'\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"‚èπÔ∏è Scraping interrupted by user\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"üí• Fatal error: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
