{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests\n",
    "%pip install bs4\n",
    "%pip install tqdm\n",
    "%pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from tqdm import tqdm\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import sys\n",
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('scraper.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "visited_urls = set()\n",
    "\n",
    "class WebScraperWithVector:\n",
    "    def __init__(self, collection_name=\"web_content\", persist_directory=\"./chroma_db\"):\n",
    "        \"\"\"Initialize the scraper with ChromaDB integration\"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self.setup_chromadb()\n",
    "        \n",
    "    def setup_chromadb(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            logger.info(\"üîß Initializing ChromaDB...\")\n",
    "            \n",
    "            # Create persistent directory\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            \n",
    "            # Initialize ChromaDB client with persistence\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            try:\n",
    "                self.collection = self.client.get_collection(name=self.collection_name)\n",
    "                logger.info(f\"üìö Found existing collection: {self.collection_name}\")\n",
    "                logger.info(f\"üìä Collection contains {self.collection.count()} documents\")\n",
    "            except:\n",
    "                self.collection = self.client.create_collection(\n",
    "                    name=self.collection_name,\n",
    "                    metadata={\"description\": \"Web scraped content with semantic search\"}\n",
    "                )\n",
    "                logger.info(f\"üÜï Created new collection: {self.collection_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to initialize ChromaDB: {e}\")\n",
    "            raise\n",
    "\n",
    "    def fetch_html(self, url):\n",
    "        \"\"\"Fetch HTML content from URL\"\"\"\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "            }\n",
    "            response = requests.get(url, timeout=10, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to fetch {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def is_valid_url(self, url):\n",
    "        \"\"\"Check if URL is valid and not blacklisted\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        blacklisted_domains = [\"linkedin.com\", \"facebook.com\", \"twitter.com\", \"instagram.com\"]\n",
    "        return (parsed.scheme in [\"http\", \"https\"] and \n",
    "                not any(domain in parsed.netloc for domain in blacklisted_domains))\n",
    "\n",
    "    def extract_links(self, soup, base_url):\n",
    "        \"\"\"Extract all valid links from the page\"\"\"\n",
    "        links = set()\n",
    "        for tag in soup.find_all(\"a\", href=True):\n",
    "            href = tag.get(\"href\")\n",
    "            full_url = urljoin(base_url, href)\n",
    "            if self.is_valid_url(full_url):\n",
    "                links.add(full_url)\n",
    "        return links\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text content\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        # Remove extra whitespace and normalize\n",
    "        return ' '.join(text.split()).strip()\n",
    "\n",
    "    def parse_content(self, url):\n",
    "        \"\"\"Parse content from a single URL\"\"\"\n",
    "        html = self.fetch_html(url)\n",
    "        if html is None:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "\n",
    "        # Extract structured data\n",
    "        data = {\n",
    "            \"url\": url,\n",
    "            \"title\": self.clean_text(soup.title.string) if soup.title and soup.title.string else \"\",\n",
    "            \"headings\": [self.clean_text(h.get_text()) for h in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']) if h.get_text().strip()],\n",
    "            \"paragraphs\": [self.clean_text(p.get_text()) for p in soup.find_all(\"p\") if p.get_text().strip()],\n",
    "            \"body_text\": self.clean_text(soup.get_text()),\n",
    "            \"images\": [{\"src\": urljoin(url, img.get(\"src\")), \"alt\": img.get(\"alt\", \"\")} \n",
    "                      for img in soup.find_all(\"img\") if img.get(\"src\")],\n",
    "            \"links\": list(self.extract_links(soup, url)),\n",
    "            \"scraped_at\": datetime.now().isoformat(),\n",
    "            \"content_hash\": None\n",
    "        }\n",
    "        \n",
    "        # Generate content hash for deduplication\n",
    "        content_for_hash = f\"{data['title']}{' '.join(data['headings'])}{' '.join(data['paragraphs'])}\"\n",
    "        data[\"content_hash\"] = hashlib.md5(content_for_hash.encode()).hexdigest()\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def prepare_document_for_vector_db(self, page_data):\n",
    "        \"\"\"Prepare document content and metadata for ChromaDB\"\"\"\n",
    "        # Combine all text content for embedding\n",
    "        text_content = []\n",
    "        \n",
    "        if page_data.get(\"title\"):\n",
    "            text_content.append(f\"Title: {page_data['title']}\")\n",
    "            \n",
    "        if page_data.get(\"headings\"):\n",
    "            text_content.append(f\"Headings: {' | '.join(page_data['headings'])}\")\n",
    "            \n",
    "        if page_data.get(\"paragraphs\"):\n",
    "            # Limit paragraph content to avoid token limits\n",
    "            paragraphs = page_data['paragraphs'][:10]  # First 10 paragraphs\n",
    "            text_content.append(f\"Content: {' '.join(paragraphs)}\")\n",
    "        \n",
    "        document_text = ' '.join(text_content)\n",
    "        \n",
    "        # Prepare metadata\n",
    "        metadata = {\n",
    "            \"url\": page_data[\"url\"],\n",
    "            \"title\": page_data.get(\"title\", \"\")[:100],  # Truncate for metadata limits\n",
    "            \"scraped_at\": page_data[\"scraped_at\"],\n",
    "            \"content_hash\": page_data[\"content_hash\"],\n",
    "            \"num_headings\": len(page_data.get(\"headings\", [])),\n",
    "            \"num_paragraphs\": len(page_data.get(\"paragraphs\", [])),\n",
    "            \"num_images\": len(page_data.get(\"images\", [])),\n",
    "            \"num_links\": len(page_data.get(\"links\", []))\n",
    "        }\n",
    "        \n",
    "        return document_text, metadata\n",
    "\n",
    "    def insert_to_vector_db(self, structured_data):\n",
    "        \"\"\"Insert scraped data into ChromaDB\"\"\"\n",
    "        logger.info(\"üíæ Inserting data into vector database...\")\n",
    "        \n",
    "        documents = []\n",
    "        metadatas = []\n",
    "        ids = []\n",
    "        \n",
    "        for i, page_data in enumerate(tqdm(structured_data, desc=\"Preparing documents\")):\n",
    "            try:\n",
    "                document_text, metadata = self.prepare_document_for_vector_db(page_data)\n",
    "                \n",
    "                # Skip empty documents\n",
    "                if not document_text.strip():\n",
    "                    logger.warning(f\"‚ö†Ô∏è Skipping empty document: {page_data.get('url', 'Unknown')}\")\n",
    "                    continue\n",
    "                \n",
    "                # Generate unique ID\n",
    "                doc_id = f\"doc_{page_data['content_hash']}_{i}\"\n",
    "                \n",
    "                documents.append(document_text)\n",
    "                metadatas.append(metadata)\n",
    "                ids.append(doc_id)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Error preparing document {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not documents:\n",
    "            logger.warning(\"‚ö†Ô∏è No valid documents to insert\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Insert in batches to avoid memory issues\n",
    "            batch_size = 50\n",
    "            total_inserted = 0\n",
    "            \n",
    "            for i in range(0, len(documents), batch_size):\n",
    "                batch_docs = documents[i:i+batch_size]\n",
    "                batch_metas = metadatas[i:i+batch_size]\n",
    "                batch_ids = ids[i:i+batch_size]\n",
    "                \n",
    "                self.collection.add(\n",
    "                    documents=batch_docs,\n",
    "                    metadatas=batch_metas,\n",
    "                    ids=batch_ids\n",
    "                )\n",
    "                \n",
    "                total_inserted += len(batch_docs)\n",
    "                logger.info(f\"üìù Inserted batch: {len(batch_docs)} documents (Total: {total_inserted})\")\n",
    "            \n",
    "            logger.info(f\"‚úÖ Successfully inserted {total_inserted} documents into ChromaDB\")\n",
    "            logger.info(f\"üìä Total collection size: {self.collection.count()} documents\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to insert documents: {e}\")\n",
    "            raise\n",
    "\n",
    "    def search_similar_content(self, query, n_results=5):\n",
    "        \"\"\"Search for similar content in the vector database\"\"\"\n",
    "        try:\n",
    "            results = self.collection.query(\n",
    "                query_texts=[query],\n",
    "                n_results=n_results\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"üîç Found {len(results['documents'][0])} results for query: '{query}'\")\n",
    "            \n",
    "            for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
    "                print(f\"\\n--- Result {i+1} ---\")\n",
    "                print(f\"URL: {metadata['url']}\")\n",
    "                print(f\"Title: {metadata['title']}\")\n",
    "                print(f\"Content Preview: {doc[:200]}...\")\n",
    "                print(f\"Scraped: {metadata['scraped_at']}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Search failed: {e}\")\n",
    "\n",
    "    def crawl_website(self, start_url, max_depth=2, max_pages=None):\n",
    "        \"\"\"Crawl website and collect structured data\"\"\"\n",
    "        structured_data = []\n",
    "        to_visit = [(start_url, 0)]\n",
    "        visited_urls.add(start_url)\n",
    "        pages_processed = 0\n",
    "\n",
    "        logger.info(f\"üöÄ Starting crawl from: {start_url}\")\n",
    "        logger.info(f\"üìê Max depth: {max_depth}, Max pages: {max_pages or 'unlimited'}\")\n",
    "\n",
    "        while to_visit and (max_pages is None or pages_processed < max_pages):\n",
    "            current_url, depth = to_visit.pop(0)\n",
    "            if depth > max_depth:\n",
    "                continue\n",
    "\n",
    "            logger.info(f\"üîç Parsing [depth {depth}]: {current_url}\")\n",
    "            page_data = self.parse_content(current_url)\n",
    "            \n",
    "            if page_data:\n",
    "                structured_data.append(page_data)\n",
    "                pages_processed += 1\n",
    "                \n",
    "                # Add new links to visit queue\n",
    "                new_links = 0\n",
    "                for link in page_data[\"links\"]:\n",
    "                    if link not in visited_urls and urlparse(link).netloc == urlparse(start_url).netloc:\n",
    "                        visited_urls.add(link)\n",
    "                        to_visit.append((link, depth + 1))\n",
    "                        new_links += 1\n",
    "                \n",
    "                logger.info(f\"‚úÖ Processed page (found {new_links} new links)\")\n",
    "            else:\n",
    "                logger.warning(f\"‚ö†Ô∏è Failed to process: {current_url}\")\n",
    "\n",
    "        logger.info(f\"üéØ Crawl completed: {len(structured_data)} pages processed\")\n",
    "        return structured_data\n",
    "\n",
    "    def save_structured_data(self, data, output_dir=\"scraped_data\"):\n",
    "        \"\"\"Save structured data to JSON files\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Save individual page files\n",
    "        for i, page in enumerate(data):\n",
    "            filename = os.path.join(output_dir, f\"page_{i+1}.json\")\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(page, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        # Save combined data\n",
    "        combined_file = os.path.join(output_dir, \"all_pages.json\")\n",
    "        with open(combined_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        logger.info(f\"üíæ Saved {len(data)} pages to '{output_dir}'\")\n",
    "\n",
    "    def get_collection_stats(self):\n",
    "        \"\"\"Display collection statistics\"\"\"\n",
    "        try:\n",
    "            count = self.collection.count()\n",
    "            logger.info(f\"üìä Collection Statistics:\")\n",
    "            logger.info(f\"   Collection Name: {self.collection_name}\")\n",
    "            logger.info(f\"   Total Documents: {count}\")\n",
    "            logger.info(f\"   Storage Path: {self.persist_directory}\")\n",
    "            \n",
    "            if count > 0:\n",
    "                # Get a sample document to show structure\n",
    "                sample = self.collection.peek(limit=1)\n",
    "                if sample['metadatas']:\n",
    "                    logger.info(f\"   Sample Metadata Keys: {list(sample['metadatas'][0].keys())}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to get collection stats: {e}\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Web Scraper with ChromaDB Vector Storage\")\n",
    "    parser.add_argument(\"--url\", required=True, help=\"Starting URL to scrape\")\n",
    "    parser.add_argument(\"--depth\", type=int, default=2, help=\"Maximum crawl depth\")\n",
    "    parser.add_argument(\"--max-pages\", type=int, help=\"Maximum pages to scrape\")\n",
    "    parser.add_argument(\"--collection\", default=\"web_content\", help=\"ChromaDB collection name\")\n",
    "    parser.add_argument(\"--db-path\", default=\"./chroma_db\", help=\"ChromaDB storage path\")\n",
    "    parser.add_argument(\"--output-dir\", default=\"scraped_data\", help=\"Output directory for JSON files\")\n",
    "    parser.add_argument(\"--search\", help=\"Search query to test vector database\")\n",
    "    parser.add_argument(\"--stats\", action=\"store_true\", help=\"Show collection statistics\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    try:\n",
    "        # Initialize scraper\n",
    "        scraper = WebScraperWithVector(\n",
    "            collection_name=args.collection,\n",
    "            persist_directory=args.db_path\n",
    "        )\n",
    "        \n",
    "        # Show stats if requested\n",
    "        if args.stats:\n",
    "            scraper.get_collection_stats()\n",
    "            return\n",
    "        \n",
    "        # Perform search if query provided\n",
    "        if args.search:\n",
    "            scraper.search_similar_content(args.search)\n",
    "            return\n",
    "        \n",
    "        # Crawl website\n",
    "        logger.info(\"üåê Starting web scraping process...\")\n",
    "        structured_data = scraper.crawl_website(\n",
    "            start_url=args.url,\n",
    "            max_depth=args.depth,\n",
    "            max_pages=args.max_pages\n",
    "        )\n",
    "        \n",
    "        if not structured_data:\n",
    "            logger.warning(\"‚ö†Ô∏è No data was scraped\")\n",
    "            return\n",
    "        \n",
    "        # Save to files\n",
    "        scraper.save_structured_data(structured_data, args.output_dir)\n",
    "        \n",
    "        # Insert into vector database\n",
    "        scraper.insert_to_vector_db(structured_data)\n",
    "        \n",
    "        # Show final statistics\n",
    "        scraper.get_collection_stats()\n",
    "        \n",
    "        logger.info(\"üéâ Scraping and vector storage completed successfully!\")\n",
    "        logger.info(f\"üí° Try searching with: python script.py --search 'your query here'\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"‚èπÔ∏è Scraping interrupted by user\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"üí• Fatal error: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
